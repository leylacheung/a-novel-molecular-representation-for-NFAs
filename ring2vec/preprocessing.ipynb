{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def format_features(df):\n",
    "    \"\"\"\n",
    "    Formats the features in the 'Features' column of the dataframe.\n",
    "    Converts each string of features into a comma-separated string after cleaning it.\n",
    "    Converts numbers from scientific notation to normal float representation.\n",
    "    \"\"\"\n",
    "    formatted_features = []\n",
    "\n",
    "    for feature_str in df['Features']:\n",
    "        if feature_str is not None and feature_str != \"\":\n",
    "            # Remove unwanted characters like brackets and newlines, then split by spaces\n",
    "            cleaned_feature_str = feature_str.replace('[', '').replace(']', '').replace('\\n', '').replace('  ', ' ')\n",
    "            # Split the cleaned string into individual numbers\n",
    "            features = cleaned_feature_str.split()\n",
    "\n",
    "            # Convert each feature to float and format it to avoid scientific notation\n",
    "            formatted_features_list = [f\"{float(feature):.5f}\" for feature in features]\n",
    "            # Join the formatted features with a comma\n",
    "            formatted_feature = ','.join(formatted_features_list)\n",
    "        else:\n",
    "            # Handle the case where features are None or empty\n",
    "            formatted_feature = ''\n",
    "\n",
    "        formatted_features.append(formatted_feature)\n",
    "\n",
    "    df['Features'] = formatted_features\n",
    "    return df\n",
    "\n",
    "\n",
    "# Usage example.\n",
    "df = pd.read_csv('data/GCN_NFAs.csv')\n",
    "df = format_features(df)\n",
    "df.to_csv('data/GCN_NFAs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_vector_string(vector_string):\n",
    "    \"\"\"Process a single vector_string to retain each number with five decimal places\"\"\"\n",
    "    numbers = vector_string.split(',')\n",
    "    formatted_numbers = [f\"{float(num):.3f}\" for num in numbers]\n",
    "    return ','.join(formatted_numbers)\n",
    "\n",
    "# Read CSV file.\n",
    "df = pd.read_csv('data/NFAs_paper_prering2alt.csv')\n",
    "\n",
    "# Apply a function to process the 'vector_string' column.\n",
    "df['Features'] = df['Features'].apply(format_vector_string)\n",
    "\n",
    "# Save the processed DataFrame back to a CSV file.\n",
    "df.to_csv('data/NFAs_prering2alt.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('data/Data_GCN_prering2alt100.csv')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv('data/Data_GCN_prering2alt100.csv', encoding='latin1')  # 尝试使用 latin1 编码\n",
    "\n",
    "\n",
    "# Define a function to strip quotes from the ends and concatenate the strings\n",
    "# def concatenate_features(f1, f2):\n",
    "#     # Strip quotes from the ends of the strings\n",
    "#     f1_cleaned = f1.strip('\"')\n",
    "#     f2_cleaned = f2.strip('\"')\n",
    "#     # Concatenate with a comma\n",
    "#     return f\"{f1_cleaned},{f2_cleaned}\"\n",
    "def concatenate_features(f1, f2):\n",
    "    # Check if f1 or f2 is NaN (a float in pandas), and replace it with empty string if so\n",
    "    if pd.isna(f1):\n",
    "        f1_cleaned = ''\n",
    "    else:\n",
    "        f1_cleaned = str(f1).strip('\"')  # Convert to string and strip quotes\n",
    "\n",
    "    if pd.isna(f2):\n",
    "        f2_cleaned = ''\n",
    "    else:\n",
    "        f2_cleaned = str(f2).strip('\"')  # Convert to string and strip quotes\n",
    "\n",
    "    # Concatenate with a comma\n",
    "    return f\"{f1_cleaned},{f2_cleaned}\"\n",
    "\n",
    "# Apply the function to each row\n",
    "df['Combined_Features'] = df.apply(lambda row: concatenate_features(row['Features1'], row['Features2']), axis=1)\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "df.to_csv('data/Data_GCN_prering2alt100.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfAttention 1\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('data/GCN_NFAs_paper_prering2alt.csv')\n",
    "\n",
    "# Clean and convert string features to a list of floating-point numbers.\n",
    "def preprocess_features(x):\n",
    "    return [float(i) for i in x.replace('[', '').replace(']', '').split(',')]\n",
    "\n",
    "df['Features1'] = df['Features1'].apply(preprocess_features)\n",
    "df['Features2'] = df['Features2'].apply(preprocess_features)\n",
    "\n",
    "# Ensure that all feature lists have the same length.\n",
    "def check_and_pad_feature_list(feature_list, target_length=110):\n",
    "    if len(feature_list) < target_length:\n",
    "        return feature_list + [0.0] * (target_length - len(feature_list))\n",
    "    elif len(feature_list) > target_length:\n",
    "        return feature_list[:target_length]\n",
    "    return feature_list\n",
    "\n",
    "df['Features1'] = df['Features1'].apply(lambda x: check_and_pad_feature_list(x, 110))\n",
    "df['Features2'] = df['Features2'].apply(lambda x: check_and_pad_feature_list(x, 110))\n",
    "\n",
    "# Set the dimensionality of each feature vector.\n",
    "feature_dim = 110  \n",
    "combined_feature_dim = feature_dim * 2  \n",
    "\n",
    "# Define a self-attention module.\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads  \n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        N = queries.shape[0]\n",
    "\n",
    "        # Reshape values, keys, queries\n",
    "        values = values.reshape(N, -1, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, -1, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, -1, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, -1, self.heads * self.head_dim)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# Define a feature fusion model.\n",
    "class FeatureFusionModel(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads):\n",
    "        super(FeatureFusionModel, self).__init__()\n",
    "        self.self_attention = SelfAttention(feature_dim, num_heads)\n",
    "        self.fc = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        attention_output = self.self_attention(features, features, features)\n",
    "        fused_features = torch.mean(attention_output, dim=1)\n",
    "        return self.fc(fused_features)\n",
    "\n",
    "# Create an instance of the model.\n",
    "model = FeatureFusionModel(feature_dim=220, num_heads=4)  # Use the concatenated feature dimension.\n",
    "\n",
    "# Convert to PyTorch tensor.\n",
    "feature1_tensor = torch.tensor(df['Features1'].tolist(), dtype=torch.float32)\n",
    "feature2_tensor = torch.tensor(df['Features2'].tolist(), dtype=torch.float32)\n",
    "\n",
    "# Ensure the correct dimensionality of feature vectors.\n",
    "assert feature1_tensor.shape[1] == feature_dim and feature2_tensor.shape[1] == feature_dim, \"Feature dimensions do not match.\"\n",
    "\n",
    "# Concatenate features\n",
    "combined_features_tensor = torch.cat((feature1_tensor, feature2_tensor), dim=1)\n",
    "\n",
    "# Use model to fuse features.\n",
    "fused_features = model(combined_features_tensor)\n",
    "\n",
    "fused_features_detached = fused_features.detach().numpy()  # Convert to NumPy array.\n",
    "fused_features_list = fused_features_detached.tolist()  # Convert to Python array. \n",
    "\n",
    "# Convert each element in the list to a string for saving to CSV.\n",
    "df['Fused_Features'] = [','.join(map(str, f)) for f in fused_features_list]\n",
    "\n",
    "# Save the updated DataFrame.\n",
    "df.to_csv('data/GCN_NFAs_prering2alt_f4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfAttention 2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def expand_features(feature, target_dim=2048):\n",
    "    return [float(i) for i in feature.replace('[', '').replace(']', '').split(',')] + [0] * (target_dim - len(feature.split(',')))\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads \n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size).to(device)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        N = queries.shape[0]\n",
    "\n",
    "        values = values.reshape(N, -1, self.heads, self.head_dim).to(device)\n",
    "        keys = keys.reshape(N, -1, self.heads, self.head_dim).to(device)\n",
    "        queries = queries.reshape(N, -1, self.heads, self.head_dim).to(device)\n",
    "        \n",
    "        queries[:, -feature_dim:, :, :] *= 2  \n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, -1, self.heads * self.head_dim)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class FeatureFusionModel(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads):\n",
    "        super(FeatureFusionModel, self).__init__()\n",
    "        self.self_attention = SelfAttention(feature_dim * 3, num_heads).to(device)\n",
    "        self.fc = nn.Linear(feature_dim * 3, feature_dim * 3).to(device)\n",
    "\n",
    "    def forward(self, feature1, feature2, feature3):\n",
    "        combined_features = torch.cat((feature1, feature2, feature3), dim=1).to(device)\n",
    "        attention_output = self.self_attention(combined_features, combined_features, combined_features)\n",
    "        fused_features = torch.mean(attention_output, dim=1)\n",
    "        return self.fc(fused_features)\n",
    "\n",
    "# df = pd.read_csv('data/dataset_last_final.csv')\n",
    "df = pd.read_csv('data/dataset_last_final.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df['Features1'] = df['Features1'].apply(lambda x: expand_features(x))\n",
    "df['Features2'] = df['Features2'].apply(lambda x: expand_features(x))\n",
    "df['Features3'] = df['Features3'].apply(lambda x: expand_features(x))\n",
    "\n",
    "feature_dim = 2048\n",
    "\n",
    "model = FeatureFusionModel(feature_dim=2048, num_heads=8)\n",
    "\n",
    "if torch.cuda.device_count() < 1:\n",
    "    print(\"use\", torch.cuda.device_count(), \"个 GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "feature1_tensor = torch.tensor(df['Features1'].tolist(), dtype=torch.float32).to(device)\n",
    "feature2_tensor = torch.tensor(df['Features2'].tolist(), dtype=torch.float32).to(device)\n",
    "feature3_tensor = torch.tensor(df['Features3'].tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "fused_features = model(feature1_tensor, feature2_tensor, feature3_tensor)\n",
    "\n",
    "fused_features_detached = fused_features.detach().cpu().numpy()  \n",
    "fused_features_list = fused_features_detached.tolist()  \n",
    "\n",
    "df['Combined_Features'] = [','.join(map(str, f)) for f in fused_features_list]\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv('data/dataset_last_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('data/GCN_NFAs_paper_prering2alt.csv')\n",
    "\n",
    "# Convert string features to list of floating-point numbers.\n",
    "df['Features1'] = df['Features1'].apply(lambda x: [float(i) for i in x.split(',')])\n",
    "df['Features2'] = df['Features2'].apply(lambda x: [float(i) for i in x.split(',')])\n",
    "# df['Features3'] = df['Features3'].apply(lambda x: [float(i) for i in x.split(',')])\n",
    "# Calculate the maximum length of each feature column.\n",
    "max_length_features1 = max(len(f) for f in df['Features1'])\n",
    "max_length_features2 = max(len(f) for f in df['Features2'])\n",
    "max_length_features3 = max(len(f) for f in df['Features3'])\n",
    "print(max_length_features1)\n",
    "print(max_length_features3)\n",
    "# Pad each of the two feature lists separately.\n",
    "# df['Features1'] = df['Features1'].apply(lambda x: x + [0] * (120 - len(x)))\n",
    "# df['Features2'] = df['Features2'].apply(lambda x: x + [0] * (120 - len(x)))\n",
    "\n",
    "# Save the processed DataFrame to a CSV file.\n",
    "# df.to_csv('data/Data_GCN_prering2alt_padded.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
